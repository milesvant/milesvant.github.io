<!doctype html><html lang=en-us><head><title>AlphaGo and AlphaGo Zero | Miles VT's Blog</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=/favicon.ico><link type=text/css rel=stylesheet href=/css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="><link type=text/css rel=stylesheet href=/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","url":"https:\/\/milesvant.github.io\/posts\/alphagozero\/","name":"AlphaGo and AlphaGo Zero","author":{"@type":"Person","name":""},"description":""}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=/>about</a></li><li><a class=active href=/posts>posts</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>AlphaGo and AlphaGo Zero</h1><time datetime="2022-11-02 14:30:00 -0700 -0700" class=post__date>Nov 2 2022</time></header><article class=post__content><h2 id=introduction>Introduction<a class=anchor href=#introduction>#</a></h2><p>In 2016, the AlphaGo program developed by DeepMind defeated Go World Champion Lee Sedol in a five game match, a landmark achievement in the history of Artificial Intelligence. About a year later, Deepmind introduced AlphaGo Zero, a markedly stronger Go program trained without supervision from any human games. In this post, I will cover the internals of these two systems. I assume familiarity with the basic setup, terminology, and notation of reinforcement learning.</p><h2 id=why-is-go-hard>Why is Go hard?<a class=anchor href=#why-is-go-hard>#</a></h2><p>For the purposes of understanding the AlphaGo family, only the basic parameters of the game of Go are important. Go is a two player, deterministic, perfect information, abstract strategy game played (in professional play) on a $19$ by $19$ grid. Players alternate placing black and white stones on this grid, aiming to surround more territory on the board than their opponent. The most relevant distinguishing feature of Go among similar games are its extremely high branching factor and length. While a typical branching factor for a game of Chess is $35$, Go games generally have hundreds of possible moves in each position and last for twice as many moves. This makes the relatively simple search techniques that yield superhuman play in Chess insufficient for reaching the level of a strong amateur in Go.</p><h2 id=monte-carlo-tree-search>Monte Carlo Tree Search<a class=anchor href=#monte-carlo-tree-search>#</a></h2><p>Use of the Monte Carlo Tree Search (MCTS) algorithm was responsible for Go programs reaching the skill of a strong, but not top-level human prior to AlphaGo. It is a decision-time planning algorithm which uses a model of the environment (in this case the game of Go) to improve upon decisions made by a policy $\pi$, generally called the <em>rollout policy</em>. Asymptotically, it converges to the optimal policy and value function. It initializes a tree of state nodes, starting at the current state if the tree is empty. Each tree node maintains visit counts and action values $Q(s, a)$ as part of the search process. The tree is expanded and evaluated by iterating through four stages: <em>selection</em>, <em>expansion</em>, <em>simulation</em>, and <em>backup</em>.</p><table><thead><tr><th style=text-align:center><img src=/images/mcts.png alt></th></tr></thead><tbody><tr><td style=text-align:center><strong>Monte Carlo Tree Search, from [3]</strong></td></tr></tbody></table><h3 id=selection>Selection<a class=anchor href=#selection>#</a></h3><p>Selection traverses the tree from the root to a leaf node by following a <em>tree policy</em>, commonly $\epsilon$-greedy or UCB using the stored action values. The tree policy should sufficiently explore the states immediately following the root state, while eventually improving on the rollout policy within the tree because it exploits the empirical returns of the Monte Carlo simulations.</p><h3 id=expansion>Expansion<a class=anchor href=#expansion>#</a></h3><p>Expansion expands the search tree by adding some amount of children from the leaf node traversed to in Selection. The frequency that the tree is expanded and the number of child nodes added varies with different implementations of the algorithm.</p><h3 id=simulation>Simulation<a class=anchor href=#simulation>#</a></h3><p>Simulation evaluates the last reached node (either the leaf or an added child) using the rollout policy $\pi$. It does that by following $\pi$ until the end of the episode is reached. Note that it is not even necessary for the rollout policy to be close to optimal for MCTS to be effective, though of course it does not hurt.</p><h3 id=backup>Backup<a class=anchor href=#backup>#</a></h3><p>Backup updates the internal state of the tree using the return(s) from the simulated episode. In most cases this simply entails incrementing the visit count for each visited node and adding the return(s) to the running average of each node&rsquo;s action value function.</p><h2 id=alphago>AlphaGo<a class=anchor href=#alphago>#</a></h2><p>A common approach taken by strong computer Go programs prior to AlphaGo was to train a policy to predict expert human moves, then use that policy in MCTS. AlphaGo takes a similar approach, but uses pipeline of several deep convolutional networks in the MCTS instead the shallower models common in earlier efforts.</p><h3 id=supervised-learning>Supervised Learning<a class=anchor href=#supervised-learning>#</a></h3><p>AlphaGo uses two policies trained by supervised learning, which they refer to as the &ldquo;Rollout policy&rdquo; $p_{\pi}$ and the &ldquo;SL policy network&rdquo; $p_{\sigma}$. Both are trained to predict expert human moves using millions of positions from an online Go server, augmented by including all 8 symmetries of each position.
$p_{\pi}$ is quite similar to the policies used in prior Go programs in that it is a linear model trained on human-designed features. It is intended to guide the MCTS simulations, so fast inference (2 $\mu$s vs 3 ms for the neural networks) was prioritized over accuracy.
$p_{\sigma}$ is a 13 layer convolutional neural network that achieved what was at the time state of the art test classification accuracy on the dataset used.</p><p>The input to the SL policy network, as well as the other two networks trained later in the pipeline, is a set of $48$ $19$ by $19$ feature planes encoding the game position as well as some basic Go knowledge.</p><table><thead><tr><th style=text-align:center><img src=/images/alphagofeatures.png alt></th></tr></thead><tbody><tr><td style=text-align:center><strong>Feature Planes for the AlphaGo Neural Networks, from [1]</strong></td></tr></tbody></table><h3 id=reinforcement-learning>Reinforcement Learning<a class=anchor href=#reinforcement-learning>#</a></h3><p>The next step of the pipeline trains a &ldquo;RL policy network&rdquo; $p_{\rho}$ by fine-tuning $p_{\sigma}$ using policy gradient reinforcement learning and self-play.
$p_{\rho}$ is trained by playing games against previous iterations of itself instead of its current iteration to prevent overfitting to the current policy. Given a game with outcome $z = \pm 1$, the network weights are updated according to REINFORCE:
$$
\Delta \rho \propto \frac{\partial \text{log }p_{\rho}(a_t | s_t)}{\partial \rho}z
$$
Using just the fine-tuned policy $p_{\rho}$ and no search led to play of comparable or greater strength than the top Go programs available at the time.</p><h3 id=value-network>Value Network<a class=anchor href=#value-network>#</a></h3><p>The final network is trained to approximate the value function $v^{\rho}(s)$ of the RL policy network.
This is done via supervised learning, with the value network taking a position from a RL policy self play game and predicting the outcome $z$.
Weights are updated by stochastic gradient descent on the MSE:
$$
\Delta \theta \propto \frac{\partial v_{\theta}(s)}{\partial \theta}(z - v_{\theta}(s))
$$
An important implementation detail is that each training example is taken from a distinct game, as successive positions from the same game are highly correlated and lead to overfitting.</p><h3 id=bringing-it-all-together-with-mcts>Bringing it all together with MCTS<a class=anchor href=#bringing-it-all-together-with-mcts>#</a></h3><p>AlphaGo brings together the trained networks into a complete system using MCTS. As is standard, each node in the search tree maintains a visit count $N(s, a)$ and an estimated action value $Q(s, a)$. In addition, each node stores prior probabilities $P(s, a)$, which are output by the SL policy network $p_{\sigma}$ when the node is expanded in the search.
Notably, the SL policy is used to compute the prior probabilities instead of the stronger RL policy, as this surprisingly lead to empirically stronger play. The authors of AlphaGo hypothesize that this is due to the SL policy selecting a more diverse beam of moves.
The tree policy initially defaults to using the prior probabilities to guide search, but over time lends more weight to the action values estimated through the Monte Carlo simulations:
$$
\begin{aligned}
a_t = &\text{ argmax }(Q(s_t, a) + u(s_t, a)) \cr
\text{where } &u(s, a) \propto \frac{P(s, a)}{1 + N(s, a)}
\end{aligned}
$$</p><p>Evaluation of the leaf nodes is slightly non-standard in that is done using both the result of a simulation $z_L$ and the value network:
$$
V(s_L) = (1 - \lambda)v_{\theta}(s_L) + \lambda z_L
$$
In practice a balanced mixture $\lambda = 0.5$ yielded the strongest play, suggesting that is important to balance the more precise evaluation of the weak rollout policy via simulation and the less precise evaluation of the strong RL policy using the value network.
At the end of search, the action with the highest visit count is selected.</p><p>Combining the networks in this fashion creates a player of markedly greater strength than any system prior, with AlphaGo winning nearly 100% of its games in a tournament against the strongest available Go programs.
AlphaGo at this stage was strong enough to defeat European Champion Fan Hui 5-0 in an October 2015 match.</p><h3 id=alphago-lee>AlphaGo Lee<a class=anchor href=#alphago-lee>#</a></h3><p>The published version of AlphaGo differs slightly from the version that beat Lee Sedol in 2016. The main differences are that the size of the input was scaled up, using $256$ feature planes instead of $48$, and that the value network was trained to predict the value of positions as played by AlphaGo instead of by the RL policy network. This creates a iterable policy improvement algorithm, where successive versions of AlphaGo can be trained by learning a value network corresponding to the previous iteration. This is a step in the direction of improvement via pure self-play, which is the approach taken by AlphaGo Zero.</p><h2 id=alphago-zero>AlphaGo Zero<a class=anchor href=#alphago-zero>#</a></h2><p>AlphaGo Zero eschews the complex pipeline of networks used in AlphaGo for a single network trained purely on board positions by self-play.
It turns out this architecture is sufficient to not only exceed human skill in Go (and later in other games such as Chess & Shogi), but also yield superior play to MCTS based on human supervision.
This is despite the fact that AlphaGo Zero has no knowledge of human play or human-designed features encoding common sense strategy.
Instead, AlphaGo Zero develops its strategy organically through a combination of re-discovering human Go knowledge and developing previously unknown strategic concepts.</p><h3 id=network-architecture>Network Architecture<a class=anchor href=#network-architecture>#</a></h3><p>AlphaGo Zero uses a single convolutional neural network with 39 residual blocks. The input is a stack of $17$ $19$ by $19$ feature planes which encode the positions of each player&rsquo;s stones in the last $8$ positions, as well as a constant plane indicating whose turn it is.
Position history is needed because Go is not fully observable, with repetitions being forbidden.
The network serves as both the policy and value function, with outputs $(\vec{p}, v) = f_{\theta}(s)$ of $p_a = \mathbb{P}_{\pi}(a | s)$ for each action $a$ and $v = v^{\pi}(s)$.</p><h3 id=mcts-training-algorithm>MCTS Training Algorithm<a class=anchor href=#mcts-training-algorithm>#</a></h3><p>Self-play proceeds using MCTS, with the same tree policy as AlphaGo, however rollouts are omitted in favor of simply using the value network to evaluate leaf nodes.
After 1600 iterations of MCTS, a move is seleted proportional to an exponentiated visit count for the children of the root node:
$$
\pi_a \propto N(s, a)^{1 / \tau}
$$
where $\tau$ is a temperature controlling the amount of exploration the search does.
A game of self play with result $z$ provides datapoints $(s_t, \vec{\pi}_t, z)$ for $t \in [0, T]$.
The training algorithm samples from these datapoints and performs gradent descent on the loss
$$
l = (z - v)^2 - \vec{\pi}^T\text{log }\vec{p} + c||\theta||^2
$$
This has the effect of adjusting $v$ to more accurately value each state, and $\vec{p}$ to more closely match $\pi$, along with L2 regularization.</p><h3 id=results>Results<a class=anchor href=#results>#</a></h3><p>Within 36 hours of training, AlphaGo Zero outperformed AlphaGo Lee in terms of Elo. After 72 hours, AlphaGo Zero defeated AlphaGo Lee in 100 straight games at the same time control that AlphaGo Lee defeated Lee Sedol.
The authors of AlphaGo Zero isolate self-play reinforcement learning as the key element that allows the agent to achieve such strong results.
While the architectural differences were useful, with a supervised learning agent using AlphaGo Zero&rsquo;s architecture achieving state of the art prediction accuracy on the human Go dataset, this agent still dramatically underformed its self-play trained analogue.
This result is highly encouraging, as it suggests that pure reinforcement learning approaches are capable of achieving state of the art results in highly complex domains where human supervision may not be available.</p><h2 id=references>References<a class=anchor href=#references>#</a></h2><p>[1] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). Mastering the game of go with deep neural networks and Tree Search. Nature, 529(7587), 484–489. <a href=https://doi.org/10.1038/nature16961 target=_blank rel="noreferrer noopener">https://doi.org/10.1038/nature16961</a></p><p>[2] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., & Hassabis, D. (2017). Mastering the game of go without human knowledge. Nature, 550(7676), 354–359. <a href=https://doi.org/10.1038/nature24270 target=_blank rel="noreferrer noopener">https://doi.org/10.1038/nature24270</a></p><p>[3] Sutton, R. S., Bach, F., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press Ltd.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script></article><div class=pagination><a class=pagination__item href=https://milesvant.github.io/posts/diffusion/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>An Introduction to Diffusion Models</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link title=GitHub href=https://github.com/milesvant target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/github.svg)></div></a><a class=social-icons__link title=Email href=mailto:milesvant@gmail.com target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/email.svg)></div></a><a class=social-icons__link title=LinkedIn href=https://www.linkedin.com/in/miles-van-tongeren-55270b119/ target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/linkedin.svg)></div></a></div><p></p></footer></div></div><div class=toc-container><div class=toc-post-title>AlphaGo and AlphaGo Zero</div><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#why-is-go-hard>Why is Go hard?</a></li><li><a href=#monte-carlo-tree-search>Monte Carlo Tree Search</a><ul><li><a href=#selection>Selection</a></li><li><a href=#expansion>Expansion</a></li><li><a href=#simulation>Simulation</a></li><li><a href=#backup>Backup</a></li></ul></li><li><a href=#alphago>AlphaGo</a><ul><li><a href=#supervised-learning>Supervised Learning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#value-network>Value Network</a></li><li><a href=#bringing-it-all-together-with-mcts>Bringing it all together with MCTS</a></li><li><a href=#alphago-lee>AlphaGo Lee</a></li></ul></li><li><a href=#alphago-zero>AlphaGo Zero</a><ul><li><a href=#network-architecture>Network Architecture</a></li><li><a href=#mcts-training-algorithm>MCTS Training Algorithm</a></li><li><a href=#results>Results</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div></main><script src=/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script>
<script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script>
<script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script>
<script src=/js/table-of-contents.js></script></body></html>