<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Miles VT's Blog</title><link>https://milesvant.github.io/posts/</link><description>Recent content in Posts on Miles VT's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 16 Dec 2022 16:10:00 -0700</lastBuildDate><atom:link href="https://milesvant.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Solving Poker using Counterfactual Regret Minimization</title><link>https://milesvant.github.io/posts/cfr/</link><pubDate>Fri, 16 Dec 2022 16:10:00 -0700</pubDate><guid>https://milesvant.github.io/posts/cfr/</guid><description>Introduction Extensive games with imperfect information are a game theoretic model used to represent sequential decision making in situtations involving other, potentially adversarial, agents with differing knowledge of the game state. This is a natural way to model many problems in domains of practical interest such as cybersecurity and auctions, as well as many commonly played recreational games, such as Poker. Research into playing game-theoretically optimal Poker has driven much of the state of the art in extensive game research due to the game&amp;rsquo;s prominence and difficulty.</description></item><item><title>AlphaGo and AlphaGo Zero</title><link>https://milesvant.github.io/posts/alphagozero/</link><pubDate>Wed, 02 Nov 2022 14:30:00 -0700</pubDate><guid>https://milesvant.github.io/posts/alphagozero/</guid><description>Introduction In 2016, the AlphaGo program developed by DeepMind defeated Go World Champion Lee Sedol in a five game match, a landmark achievement in the history of Artificial Intelligence. About a year later, Deepmind introduced AlphaGo Zero, a markedly stronger Go program trained without supervision from any human games. In this post, I will cover the internals of these two systems. I assume familiarity with the basic setup, terminology, and notation of reinforcement learning.</description></item><item><title>An Introduction to Diffusion Models</title><link>https://milesvant.github.io/posts/diffusion/</link><pubDate>Thu, 27 Oct 2022 16:26:37 -0700</pubDate><guid>https://milesvant.github.io/posts/diffusion/</guid><description>Introduction Denoising Diffusion Probabilistic Models (&amp;ldquo;DDPMs&amp;rdquo; or just &amp;ldquo;Diffusion models&amp;rdquo;) have garnered significant interest as of late due to their use in the recent slate of amazing text-to-image models such as DALLÂ·E 2 and Stable Diffusion. In this post I will explain the basic mechanisms and math behind how these models work.
DDPMs (Sohl-Dickstein et al., 2015) are probabilistic generative models, meaning that they aim to learn an approximation for a target data distribution $q(x^{(0)})$.</description></item></channel></rss>