<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Miles VT's Blog</title><link>https://milesvant.github.io/posts/</link><description>Recent content in Posts on Miles VT's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Nov 2022 14:30:00 -0700</lastBuildDate><atom:link href="https://milesvant.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>AlphaGo and AlphaGo Zero</title><link>https://milesvant.github.io/posts/alphagozero/</link><pubDate>Wed, 02 Nov 2022 14:30:00 -0700</pubDate><guid>https://milesvant.github.io/posts/alphagozero/</guid><description>Introduction In 2016, the AlphaGo program developed by DeepMind defeated Go World Champion Lee Sedol in a five game match, a landmark achievement in the history of Artificial Intelligence. About a year later, Deepmind introduced AlphaGo Zero, a markedly stronger Go program trained without supervision from any human games. In this post, I will cover the internals of these two systems. I assume familiarity with the basic setup, terminology, and notation of reinforcement learning.</description></item><item><title>An Introduction to Diffusion Models</title><link>https://milesvant.github.io/posts/diffusion/</link><pubDate>Thu, 27 Oct 2022 16:26:37 -0700</pubDate><guid>https://milesvant.github.io/posts/diffusion/</guid><description>Introduction Denoising Diffusion Probabilistic Models (&amp;ldquo;DDPMs&amp;rdquo; or just &amp;ldquo;Diffusion models&amp;rdquo;) have garnered significant interest as of late due to their use in the recent slate of amazing text-to-image models such as DALLÂ·E 2 and Stable Diffusion. In this post I will explain the basic mechanisms and math behind how these models work.
DDPMs (Sohl-Dickstein et al., 2015) are probabilistic generative models, meaning that they aim to learn an approximation for a target data distribution $q(x^{(0)})$.</description></item></channel></rss>