<!doctype html><html lang=en-us><head><title>Solving Poker using Counterfactual Regret Minimization | Miles VT's Blog</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=/favicon.ico><link type=text/css rel=stylesheet href=/css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="><link type=text/css rel=stylesheet href=/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","url":"https:\/\/milesvant.github.io\/posts\/cfr\/","name":"Solving Poker using Counterfactual Regret Minimization","author":{"@type":"Person","name":""},"description":""}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=/>about</a></li><li><a class=active href=/posts>posts</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>Solving Poker using Counterfactual Regret Minimization</h1><time datetime="2022-12-16 16:10:00 -0700 -0700" class=post__date>Dec 16 2022</time></header><article class=post__content><h2 id=introduction>Introduction<a class=anchor href=#introduction>#</a></h2><p>Extensive games with imperfect information are a game theoretic model used to represent sequential decision making in situtations involving other, potentially adversarial, agents with differing knowledge of the game state. This is a natural way to model many problems in domains of practical interest such as cybersecurity and auctions, as well as many commonly played recreational games, such as Poker.
Research into playing game-theoretically optimal Poker has driven much of the state of the art in extensive game research due to the game&rsquo;s prominence and difficulty.
The huge game tree size of the most common Poker variations poses a significant challenge for computer play, requiring the development of many novel techniques to compress and approximate the game equilibrium.
This post will cover the game theory concepts used to model Poker and its optimal play, as well as the most prominent algorithm used in computer Poker, Counterfactual Regret Minimization.</p><h2 id=extensive-games-with-imperfect-information>Extensive Games with Imperfect Information<a class=anchor href=#extensive-games-with-imperfect-information>#</a></h2><p>The notion of an <em>extensive game with imperfect information</em> is a theoretical model commonly used for analyzing multiplayer sequential games with hidden information, such as Poker. It is specifically defined as comprising of the following elements:</p><ol><li>A finite set $N$ of players</li><li>A set $H$ of sequences, called <em>histories</em>, such that $\emptyset \in H$, and $H$ is closed with respect to prefixes. If $h \in H$ is not the prefix of any other history in $H$, it is called <em>terminal</em>, and each non-terminal history $h$ has an associated action set $A(h) = \lbrace a: (h, a) \in H \rbrace$. The set of terminal histories is called $Z$.</li><li>A function $P: H \backslash Z \to N \cup \lbrace c \rbrace$, called the <em>player function</em>. If $P(h) = c$, chance determines the action taken after $h$. For each $h$ with $P(h) = c$, there is an associated probability measure $f_c(\cdot | h)$ that determines the probability of each action taken after $h$.</li><li>For each player $i \in N$ a partition $\mathcal{I}_i$ of $\lbrace h \in H : P(h) = i \rbrace$ with the property that $A(h) = A(h&rsquo;)$ for all $h, h&rsquo;$ in the same partition. This defines an associated action set $A(I_i)$ for each partition $I_i \in \mathcal{I}_i$. $\mathcal{I}_i$ is called the <em>information partition</em> for player $i$ and each $I_i \in \mathcal{I}_i$ is an <em>information set</em>.</li><li>For each player $i \in N$ there is a utility function $u_i: Z \to \mathbb{R}$.</li></ol><p>The key element that represents hidden information in the games studied with this model is the notion of information partitions for each player. Histories that in the same information set for a specific player are indistinguishable with regards to that player, so they are used to represent game states where hidden information differs but public information does not.</p><p>Now that we&rsquo;ve defined games, we can define strategies. A strategy for player $i$ is defined as a function $\sigma_i : \mathcal{I}_i \to p(\cdot | I_i)$, mapping each information set to a probability distribution over potential actions. The set of strategies for a player $i$ is called $\Sigma_i$. A <em>strategy profile</em> $\sigma$ in a multiplayer game is a strategy for each player, and $\sigma_{-i}$ refers to all strategies in a profile except for $\sigma_i$. $\pi^{\sigma}(h)$ is the probability of $h$ occurring if players follow $\sigma$, with $\pi^{\sigma}_i(h)$ being player $i$&rsquo;s contribution to this probability and $\pi^{\sigma}_{-i}(h)$ defined as expected. We define similar probabilities $\pi^{\sigma}_i(I)$ for information sets by summing over all histories in $I$. Using these definitions we can define the utility of a strategy profile for each player as $u_i(\sigma) = \mathbb{E}_{\pi^{\sigma}} \left[ u_i(h) \right]$. Define $u_i(\sigma_{-i}, \sigma_i&rsquo;)$ as the expected utility for player $i$ when all other players play according to $\sigma$ and $i$ plays according to $\sigma_i&rsquo;$.</p><h2 id=nash-equilibria>Nash Equilibria<a class=anchor href=#nash-equilibria>#</a></h2><p>The notion of a <em>Nash Equilibrium</em> is perhaps the most important concept in Game Theory. Given a strategy profile $\sigma$, $i$&rsquo;s <em>best response</em> to $\sigma$ is $\text{br}_i(\sigma) = \text{argmax}_{\sigma*_i \in \Sigma_i} u_i(\sigma_{-i}, \sigma^*_i)$.
This is the strategy that maximizes $i$&rsquo;s utility given that their opponents are fixed to following $\sigma$. A strategy profile $\sigma$ is a <em>Nash Equilibrium</em> if each $\sigma_i$ is $i$&rsquo;s best response to $\sigma$. If players are following a Nash equilbirium strategy profile, no player has an incentive to deviate from their strategy as they are already maximizing their expected utility with regards to the other players.</p><p>Games can have multiple Nash Equilibria, and every finite game will have at least one. In two player games, Nash equilibria are particularly powerful since any player who follows such a strategy is not guaranteed to not lose in expectation to any opponent (beyond loss guaranteed by the intrinsic advantage in game rules). This is not the case in multiplayer games, where if each player chooses a strategy from a set of Nash equilibria, the resulting joint strategy may not itself be an equilibrium.</p><p>In practical applications the notion of an approximate equilibrium is useful. A strategy profile $\sigma$ is an $\epsilon$-Nash Equilibrium if $u_i(\text{br}_i(\sigma)) - u_i(\sigma_i) \le \epsilon$ for each player $i$. A common goal when creating agents that play two player games is to find a $\epsilon$-Nash Equilibrium for a reasonably small value of $\epsilon$. Given the above discussion on the optimality of choosing an equilibrium strategy in two player games, this if achieved is usually sufficient for defeating even highly skilled human opposition.</p><h2 id=regret>Regret<a class=anchor href=#regret>#</a></h2><p>The concept of <em>regret</em> is central in online learning problems and is commonly applied to the game theory setting. Informally, in repeated iterations of a game, we define the regret of a sequence of strategies as the total utility lost by using that sequence rather than the optimal fixed strategy in hindsight. Formally, the <em>average overall regret</em> of player $i$ after $T$ iterations of playing a (possibly non-fixed) strategy profile $\sigma^t$ is defined as
$$
R^T_i = \frac{1}{T} \text{max}_{\sigma^*_i \in \Sigma_i} \sum_{t = 1}^T (u_i(\sigma^{*}_i, \sigma^t_{-i}) - u_i(\sigma^t))
$$
Given a sequence of strategy profiles $\sigma_t$, we can define an <em>average strategy</em> that encapsulates the average behavior over all timesteps:
$$
\bar{\sigma}^t_i(I)(a) = \frac{\sum_{t=1}^T \pi_i^{\sigma^t}(I) \sigma^t_i(I)(a)}{\sum_{t=1}^T \pi_i^{\sigma^t}(I)}
$$
There is a natural connection between regret and Nash Equilibria, which is that in a two player zero sum game where both players have average overall regrets of less than $\epsilon$, the players&rsquo; average strategy is a $2\epsilon$ Nash-equilbrium. Notably, this means that if we can generate a sequence of strategy profiles with sublinear total regret, in the limit the average strategy of this sequence is a Nash equilibrium.
A crucial detail here is that it is the average strategy profile that converges to a Nash equilbrium, not the strategy profile in the limit, which may never converge.</p><h2 id=regret-matching>Regret Matching<a class=anchor href=#regret-matching>#</a></h2><p><em>Regret Matching</em> is just such an algorithm to generate a sequence of strategy profiles with sublinear total regret.
The regret matching algorithm at each time step assigns a strategy profile where actions are taken proportional to the average regret up until that time, compared against a pure strategy of always selecting that action (rather than the optimal strategy as in the above section). This algorithm generates a sequence of strategy profiles with total regret proportional to $\sqrt{T}$, meaning that in the limit the average strategy converges to an approximate Nash Equilibrium.</p><p>Regret Matching works when the game being played is represented in normal form, so as a single round game with an associated payoff matrix. In the case of an extensive game this would mean that each player&rsquo;s &ldquo;actions&rdquo; correspond to complete strategies that contain each response to other players&rsquo; choices and chance events. In a game with a particularly large game tree such as most commonly played Poker variations, this is obviously intractable, so a number of more advanced techniques are needed.</p><p>I&rsquo;ve created a toy example of regret matching in action for the games of Rock-Paper-Scissors and the Prisoner&rsquo;s Dilemma, which is available on <a href=https://github.com/milesvant/blog_examples/blob/main/cfr/regret_matching.py target=_blank rel="noreferrer noopener">Github</a>.</p><h2 id=game-abstraction>Game Abstraction<a class=anchor href=#game-abstraction>#</a></h2><table><thead><tr><th style=text-align:center><img src=/images/game_abstraction.png alt></th></tr></thead><tbody><tr><td style=text-align:center><strong>Game Abstraction, from [3]</strong></td></tr></tbody></table><p><em>Game Abstraction</em> is a technique to solve for approximate equilibria in large games. The general procedure is to construct a smaller game that is strategically similar to the large game, solve for a small game equilibrium, then transfer that equilibrium back up to the larger game. If the abstracted game is chosen carefully, then the equilibrium found there should still be an approximate equilibrium in the full size game. Game abstractions are selected both by hand using domain expertise as well as by some automated processes. It is generally the case (and logical) that larger abstractions transfer better to the full size game, so developing methods that can efficiently solve for equilibria in large size games is still useful. The general progression of improvements in Poker AIs has been more and more efficient equilbrium finding algorithms which allow larger and larger abstractions of the full Poker game tree to be solved.</p><h2 id=counterfactual-regret-minimization>Counterfactual Regret Minimization<a class=anchor href=#counterfactual-regret-minimization>#</a></h2><p><em>Counterfactual Regret Minimization</em> (CFR) is one of the key techniques that allowed Poker abstractions to grow by several orders of magnitude. Rather than applying the regret minimization procedure on a complete strategy for the game, CFR applies Regret Minimization at each information set.
Since Regret is a notion defined over a complete strategy, we need to define an analogous metric for information sets, namely counterfactual regret.
Informally, the counterfactual regret of an action $a$ at an information set $I$ is the utility loss or gain from taking $a$ at $I$ provided that $I$ is reached.
Formally, define the counterfactual utility $u_i(\sigma, I)$ as the expected utility conditioned on $I$ being reached, with all players playing according to $\sigma$ except for player $i$ who plays in order to reach $I$.
Then the counterfactual regret of an action $a$ is defined as:
$$
R_i(I, a) = \pi_{-i}^{\sigma}(I)(u_i(\sigma_{I\rightarrow a}, I) - u_i(\sigma, I))
$$
where $\sigma_{I \rightarrow a}$ is a strategy where player $i$ follows $\sigma$ except at $I$, where they always take action $a$.
CFR then selects actions at a specfic information set with frequences proportional to their counterfactual regret at that information set.
The reason this is useful is because the total counterfactual regret across all information sets serves as an upper bound to the total (non-counterfactual) regret.
Since the CFR algorithm generates sublinear counterfactual regret growth, the average strategy yielded converges to a Nash Equilibrium as in vanilla Regret Matching.</p><h2 id=chance-sampling-and-monte-carlo-cfr>Chance Sampling and Monte Carlo CFR<a class=anchor href=#chance-sampling-and-monte-carlo-cfr>#</a></h2><p>Even the vanilla CFR algorithm is too computationally inefficient to use in large size extensive games, as calculating the counterfactual utilities still requires traversing the entire game tree.
To circumvent this, implementations of CFR for solving Poker commonly estimate the counterfactual utilities by sampling a subset of terminal histories for each information set.
This creates a common dynamic for machine learning optimization procedures (e.g. in Stochastic vs Batch gradient descent) where smaller samples increase the speed of iteration but decrease the speed of convergence due to noisier estimates.
At the extreme end is Monte Carlo CFR, where only a single terminal history is used at each iteration.</p><h2 id=references>References<a class=anchor href=#references>#</a></h2><p>[1] Lanctot, M., Waugh, K., Zinkevich, M., & Bowling, M. (2009). Monte Carlo Sampling for Regret Minimization in Extensive Games. Neural Information Processing Systems, 22, 1078–1086. <a href=https://doi.org/10.7939/r3319s48q target=_blank rel="noreferrer noopener">https://doi.org/10.7939/r3319s48q</a></p><p>[2] Martin J., O. (2022). Course In Game Theory (1st ed.). Phi.</p><p>[3] Sandholm, T. (2015). Abstraction for Solving Large Incomplete-Information Games. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1). <a href=https://doi.org/10.1609/aaai.v29i1.9757 target=_blank rel="noreferrer noopener">https://doi.org/10.1609/aaai.v29i1.9757</a></p><p>[4] Zinkevich, M., Johanson, M., Bowling, M., & Piccione, C. (2007). Regret Minimization in Games with Incomplete Information. Neural Information Processing Systems, 20, 1729–1736. <a href=https://doi.org/10.7939/r3q23r282 target=_blank rel="noreferrer noopener">https://doi.org/10.7939/r3q23r282</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script></article><div class=pagination><a class=pagination__item href=https://milesvant.github.io/posts/alphagozero/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>AlphaGo and AlphaGo Zero</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link title=GitHub href=https://github.com/milesvant target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/github.svg)></div></a><a class=social-icons__link title=Email href=mailto:milesvant@gmail.com target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/email.svg)></div></a><a class=social-icons__link title=LinkedIn href=https://www.linkedin.com/in/miles-van-tongeren-55270b119/ target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/linkedin.svg)></div></a></div><p></p></footer></div></div><div class=toc-container><div class=toc-post-title>Solving Poker using Counterfactual Regret Minimization</div><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#extensive-games-with-imperfect-information>Extensive Games with Imperfect Information</a></li><li><a href=#nash-equilibria>Nash Equilibria</a></li><li><a href=#regret>Regret</a></li><li><a href=#regret-matching>Regret Matching</a></li><li><a href=#game-abstraction>Game Abstraction</a></li><li><a href=#counterfactual-regret-minimization>Counterfactual Regret Minimization</a></li><li><a href=#chance-sampling-and-monte-carlo-cfr>Chance Sampling and Monte Carlo CFR</a></li><li><a href=#references>References</a></li></ul></nav></div></div></main><script src=/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script>
<script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script>
<script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script>
<script src=/js/table-of-contents.js></script></body></html>