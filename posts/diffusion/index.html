<!doctype html><html lang=en-us><head><title>An Introduction to Diffusion Models | Miles VT's Blog</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=/favicon.ico><link type=text/css rel=stylesheet href=/css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="><link type=text/css rel=stylesheet href=/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","url":"https:\/\/milesvant.github.io\/posts\/diffusion\/","name":"An Introduction to Diffusion Models","author":{"@type":"Person","name":""},"description":""}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=/>about</a></li><li><a class=active href=/posts>posts</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>An Introduction to Diffusion Models</h1><time datetime="2022-10-27 16:26:37 -0700 -0700" class=post__date>Oct 27 2022</time></header><article class=post__content><h2 id=introduction>Introduction<a class=anchor href=#introduction>#</a></h2><p><em>Denoising Diffusion Probabilistic Models</em> (&ldquo;DDPMs&rdquo; or just &ldquo;Diffusion models&rdquo;) have garnered significant interest as of late due to their use in the recent slate of amazing text-to-image models such as DALL·E 2 and Stable Diffusion. In this post I will explain the basic mechanisms and math behind how these models work.</p><p>DDPMs (<a href=https://arxiv.org/pdf/1503.03585.pdf target=_blank rel="noreferrer noopener">Sohl-Dickstein et al., 2015</a>) are probabilistic generative models, meaning that they aim to learn an approximation for a target data distribution $q(x^{(0)})$. This can be done for a number of reasons, for example to draw new samples from that distribution (new images similar to training images, new text similar to training corpus), or to learn a latent variable representation of the data in an unsupervised fashion. Flexible probabilistic models meant to model a wide range of distributions are frequently intractable due to issues such as computing the <em>partition function</em> $(\int \tilde{p}(x) dx)$ for an unnormalized probability distribution $\tilde{p}(x)$. A common approach to circumvent these issues is to instead learn a data generation process and follow this process to draw new samples. This is the approach taken by DDPMs.</p><h2 id=data-generation-process>Data Generation Process<a class=anchor href=#data-generation-process>#</a></h2><p>DDPMs model the data generation process as the inverse of a <em>forward diffusion process</em>, which is a Markov chain that converts the initial data distribution $q(x^{(0)})$ into a tractable prior $\pi(y)$ via repeated application of a <em>diffusion kernel</em> $$q(x^{(t)} | x^{(t-1)}) = T_t(x^{(t)} | x^{(t-1)}, \beta_t)$$</p><p>For the remainder of this post I will assume the standard choices
$$
\begin{aligned} \pi(y) &\sim \mathcal{N}(y; \textbf{0}, \textbf{I}) \cr
q(x^{(t)} | x^{(t-1)}) &\sim \mathcal{N}(x^{(t)}; \sqrt{1 - \beta_t}x^{(t-1)}, \beta_t\textbf{I})
\end{aligned}
$$</p><p>though for same target data distributions (e.g. discrete ones) this may not be appropriate. This choice of prior and kernel has the convenient property that the intermediate distributions of the noised data $q(x^{(t)} | x^{(0)})$ can be calculated in closed form, without running the Markov chain for $t$ steps:
$$
\begin{aligned}
q(x^{(t)} | x^{(0)}) &\sim \mathcal{N}(x^{(t)}; \sqrt{\tilde{\alpha}_t}x^{(0)}, \sqrt{1 - \tilde{\alpha}_t}\textbf{I}) \cr
\text{with } \tilde{\alpha}_t &:= \prod_{i=0}^t (1 - \beta_i)
\end{aligned}
$$</p><p>Note that for suitably chosen diffusion parameters $\beta_t$, as the Markov chain length $T \rightarrow \infty$, then $q(x^{(T)} | x^{(0)})$ converges in distribution to the prior $\pi(y)$ as desired.</p><p>Based on this forward process, the data generation process is conceptually straightforward. We run the Markov chain in reverse, starting with a prior sample $x^{(T)} \sim \pi(x^{(T)})$, then repeatedly drawing samples from the inverted diffusion kernels $q(x^{(t-1)} | x^{(t)})$ for $t = T &mldr; 1$ until reaching a final sample $x^{(0)}$. Of course, the exact inverted kernels $q(x^{(t-1)} | x^{(t)})$ is unavailable to us, so we must approximate them with a learned distribution $p_{\theta}(x^{(t-1)} | x^{(t)}) \approx q(x^{(t-1)} | x^{(t)})$. For small $\beta_t$, $q(x^{(t-1)} | x^{(t)})$ is also normal, meaning that we can learn $p_{\theta}$ by learning the distribution mean $\mu_{\theta}(x^{(t)}, t)$ and variance $\Sigma_{\theta}(x^{(t)}, t)$ with a function approximator such as a neural network.</p><h2 id=loss-function>Loss Function<a class=anchor href=#loss-function>#</a></h2><p>As the negative log likelihood $\mathbb{E}[-\text{log }p_{\theta}(x^{(0)})]$ is intractable, we train in standard fashion by optimizing the <em>variational lower bound</em>
$$
\mathbb{E}_{q(x^{(1&mldr;T)} | x^{(0)})} \left[ -\text{log }\frac{p_{\theta}(x^{(0&mldr;T)})}{q(x^{(1&mldr;T)} | x^{(0)})} \right] \geq -\text{log }p_{\theta}(x)
$$
I will now give the main results related to the DPPM loss function, following the calculations in <a href=https://arxiv.org/pdf/2208.11970.pdf target=_blank rel="noreferrer noopener">Luo (2022)</a>. Expanding and using the Markov property $q(x^{(t)} | x^{(t-1)}) = q(x^{(t)} | x^{(t-1)}, x^{(0)})$ yields the loss:
$$
\begin{aligned}
L_{VLB} := \text{ } &\mathbb{E}_{q(x^{(1)} | x^{(0)})} \left[ \text{log } p_{\theta}(x^{(0)} | x^{(1)}) \right] \cr
&- \sum_{t = 2}^T \mathbb{E}_{q(x^{(t)} | q^{(0)})} \left[ D_{KL}(q(x^{(t-1)} | x^{(t)}, x^{(0)}) || p_{\theta}(x^{(t-1)} | x^{(t)})) \right]
\end{aligned}
$$
The individual terms in the summation are referred to as $L_t$ in the literature and in the rest of this post.
Conditioning the inverted diffusion kernel on $x^{(0)}$ surprisingly gives it the tractable form
$$
\begin{aligned}
q(x^{(t-1)} | x^{(t)}, x^{(0)}) &= \mathcal{N}(x^{(t-1)}; \tilde{\mu}_t(x^{(t)}, x^{(0)}), \tilde{\beta}_t\textbf{I}) \cr
\text{where } \tilde{\mu}_t(x^{(t)}, x^{(0)}) &:= \frac{\sqrt{\tilde{\alpha}_{t-1}}\beta_t}{1 - \tilde{\alpha_t}}x^{(0)} + \frac{\sqrt{1 - \beta_t}(1 - \tilde{\alpha}_{t-1})}{1 - \tilde{\alpha_t}}x^{(t)} \cr
\text{and } \tilde{\beta}_t &:= \frac{1 - \tilde{\alpha}_{t-1}}{1 - \tilde{\alpha}_t}\beta_t
\end{aligned}
$$
meaning that each of the terms in the summation in $L$ are the divergence between two Gaussians and therefore easily computable, taking the form:
$$
D_{KL}(q(x^{(t-1)} | x^{(t)}, x^{(0)}) || p_{\theta}(x^{(t-1)} | x^{(t)})) = \frac{1}{2 \sigma^2_q(t)}|| \tilde{\mu}_t(x^{(t)}, x^{(0)}) - \mu_{\theta}(x^{(t)}, t)||^2
$$
where we are assuming that the approximated variance $\Sigma_{\theta}(x^{(t)}, t) = \sigma^2_q(t) \textbf{I}$ is isotropic and varies only with $t$. Given the exact form of $\tilde{\mu}_t(x^{(t)}, x^{(0)})$ above, we can parameterize our neural network to learn an approximation of the ground truth image $\hat{x}_{\theta}(x^{(t)}, t) \approx x^{(0)}$, as
$$
\begin{aligned}
D_{KL}(q(x^{(t-1)} | x^{(t)}, x^{(0)}) || p_{\theta}(x^{(t-1)} | x^{(t)})) = \frac{1}{2 \sigma^2_q(t)} \frac{\tilde{\alpha}_{t-1}\beta_t^2}{(1 - \tilde{\alpha}_t)^2} || x^{(0)} - \hat{x}_{\theta}(x^{(t)}, t)||^2
\end{aligned}
$$</p><p>Alternatively, following <a href=https://arxiv.org/pdf/2006.11239.pdf target=_blank rel="noreferrer noopener">Ho et al. (2020)</a>, the neural network can learn to predict the specific noise $\epsilon \sim \mathcal{N}(\textbf{0}, \textbf{I})$ that corrupted $x^{(t)}$ from $x^{(0)}$. In fact, they fix the variances $\sigma^2_q(t)$ and learn using a reweighted objective:
$$
L_{simple} = \mathbb{E}_{t, x_0, \epsilon} \left[ ||\epsilon - \hat{\epsilon}_{\theta}(x^{(t)}, t) ||^2 \right]
$$
which in practice led to more stable learning. The reweighted objective underweights the loss from the initial noising steps and overweights the loss from the later ones. Intuitively, this improves training because the initial steps add very little noise compared to the later ones and are as a result easier to learn. This is the formulation of the loss function most commonly used in practice. In either form, the DDPM can be seen as learning a sequence of denoising autoencoders.</p><h2 id=improvements>Improvements<a class=anchor href=#improvements>#</a></h2><p>In this section, I will cover a couple of interesting improvements to the DDPM baseline that <a href=https://arxiv.org/pdf/2102.09672.pdf target=_blank rel="noreferrer noopener">Nichol & Dhariwal (2021)</a> made. DDPMs had up to that paper yielded high-quality samples in a number of domains, such as image and audio generation, but were yet to produce competitive log-likelihoods when compared to more established generative model classes such as Variational Autoencoders. This suggested that they had poor mode coverage in comparison.</p><h3 id=learning-the-variance>Learning the Variance<a class=anchor href=#learning-the-variance>#</a></h3><p>Note that the reweighted objective $L_{simple}$ is constant with respect to the approximated variance, forcing it to be fixed prior to training. Ho et al. noticed that this produced high sample quality across a range of plausible variances (from $\beta_t$ to $\tilde{\beta}_t$), while learning the variance led to unstable learning. Nichol & Dhariwal instead learned a variance interpolated in the plausible range:
$$
\Sigma_{\theta}(x^{(t)}, t) = \text{exp} (v \text{ log }\beta_t + (1 - v) \text{ log } \tilde{\beta}_t )
$$
where $v$ is a vector outputted by the model. As $L_{simple}$ cannot guide the training of $v$, they use a hybrid objective
$$
L_{hybrid} = L_{simple} + \lambda L_{VLB}
$$
with $\lambda = 0.0001$. Because they intended for the new $L_{VLB}$ term to only guide the choice of variance, they stopped the gradient for that term from flowing to the mean approximation $\mu_{\theta}(x^{(t)}, t)$. They observed that $L_{VLB}$ is quite noisy, making optimization difficult. To reduce variance, they resampled the individual terms in $L_{VLB}$ with importance sampling:
\begin{align*}
L_{resampled} = \mathbb{E}_{t \sim p_t} \left[ \frac{L_t}{p_t} \right] \cr
\text{where } p_t \propto \sqrt{\mathbb{E} \left[ L_t^2 \right]}
\end{align*}</p><h3 id=noise-schedule>Noise Schedule<a class=anchor href=#noise-schedule>#</a></h3><p>Nichol & Dhariwal also noticed that the linear noise schedule used in Ho et al. seemed to corrupt the samples too quickly, as a large chunk of the transitions at the end of the Markov chain could be removed with little to no drop in sample quality. Instead they used a cosine noise schedule:
$$
\begin{aligned}
\tilde{\alpha}_t &= \frac{f(t)}{f(0)} \cr
\text{where } f(t) &:= \text{cos } \left( \frac{t/T + s}{1 + s} \cdot \frac{\pi}{2} \right)
\end{aligned}
$$
with $s$ a small constant added to produce sufficient noise in the early transitions.</p><h3 id=strided-ancestral-sampling>Strided Ancestral Sampling<a class=anchor href=#strided-ancestral-sampling>#</a></h3><p>In the baseline DDPM, sampling is extremely slow, as it entails running inference through the trained network once per denoising step for each new sample. The state of the art DDPMs frequently contained thousands of denoising steps, meaning that a single sample could take minutes to generate on a GPU. Nichol & Dhariwal propose a simple solution to this problem, which is to sample using a subsequence of the denoising steps. They reduce $T$ steps to $K$ by picking $K$ evenly spaced time steps and running the reverse diffusion process across those only. This enabled near-optimal metrics with up to $40$ times fewer denoising steps.</p><h2 id=conditioned-diffusion-models>Conditioned Diffusion Models<a class=anchor href=#conditioned-diffusion-models>#</a></h2><p>A natural extension to generatively modeling a data distribution $p(x)$ is to model a conditional distribution $p(x | y)$. Some example applications of this formulation are upsampling models which output a high resolution image conditioned on a low resolution version of the same image, or image synthesizers that output an image conditioned on class information or text.</p><p>A naive approach to this problem is to simply pass in the conditioning information as input to the network along with the noised samples and time step. This is common and can yield good results, but two main strategies building on this approach have been used to even greater success: classifier guidance and classifier-free guidance.</p><h3 id=classifier-guidance>Classifier Guidance<a class=anchor href=#classifier-guidance>#</a></h3><p>Logically, to train with classifier guidance we need a classifier $p_{\phi}(y | x^{(t)})$ which predicts the conditioning information. Note that even if we are modeling a dataset which has highly accurate pretrained classifiers available, such as ImageNet or CIFAR-10, these will most likely be insufficient for guidance due to poor performance on <em>noised</em> samples.</p><p>Given such a classifier, <a href=https://arxiv.org/pdf/2105.05233.pdf target=_blank rel="noreferrer noopener">Dhariwal & Nichol (2021)</a> derive an approximate form for the conditioned reverse sampling distribution:
$$
p_{\theta, \phi}(x^{(t)} | x^{(t-1)}, y) \approx \mathcal{N}(x^{(t)}; \mu_{\theta}(x^{(t)}, t) + w\Sigma_{\theta}(x^{(t)}, t)\nabla_{x^{(t)}} \text{log } p_{\phi}(y | x^{(t)}), \Sigma_{\theta}(x^{(t)}, t))
$$
Based on this form, the surprising result is that the conditioning only has to factor in at the sampling stage. By training an unconditioned diffusion model $\mu_{\theta}, \Sigma_{\theta}$ and a noisy classifier $p_{\phi}$, conditioned samples can be generated by moving the sampling mean in the direction of the classifier gradient. A scaling term $w$ has been added to govern how much classifier guidance is desired. Empirically, increasing the amount of classifier guidance trades off sample quality for sample diversity, with increasing guidance decreasing diversity.</p><h3 id=classifier-free-guidance>Classifier-Free Guidance<a class=anchor href=#classifier-free-guidance>#</a></h3><p>Classifier-Free Guidance (<a href="https://openreview.net/pdf?id=qw8AKxfYbI" target=_blank rel="noreferrer noopener">Ho & Salimans, 2021</a>) is another method that can be used to tune the exact amount of conditioning information used in the sampling process. A problem with Classifier Guidance is that it requires the separate training of a noised sample classifier alongside the diffusion model. Instead of using a classifier, Classifier-Free Guidance uses both an unconditioned diffusion model $p_{\theta}(x^{(t-1)} | x^{(t)}, t)$ and a conditioned diffusion model $p_{\phi}(x^{(t-1)} | x^{(t)}, t, y)$. The exact distribution used for sampling is then
$$
\tilde{p}(x^{(t-1)} | x^{(t)}, t, y) = wp_{\theta}(x^{(t-1)} | x^{(t)}, t) + (1-w)p_{\phi}(x^{(t-1)} | x^{(t)}, t, y)
$$
where once again $w$ is a hyperparameter that scales the exact amount of guidance used in the sampling process. This parameter displays similar control of the sample quality-diversity trade-off as its analogue in Classifier Guidance.</p><p>At first glance this seems to have the same problem as Classifier Guidance, as it looks like it requires training two distinct diffusion models. However this can be cleverly avoided by instead training a singular conditional diffusion model and randomly dropping out the conditioning information during training.</p><h3 id=comparing-classifier-and-classifier-free-guidance>Comparing Classifier and Classifier-Free Guidance<a class=anchor href=#comparing-classifier-and-classifier-free-guidance>#</a></h3><p>In large-scale systems (eg DALL·E 2, Imagen) based on conditioned diffusion models, Classifier-Free Guidance is almost exclusively used. A head to head comparison of both conditioning methods was performed in the development of GLIDE (<a href=https://arxiv.org/pdf/2112.10741.pdf target=_blank rel="noreferrer noopener">Nichol et al., 2022</a>), a text-to-image model from OpenAI. Besides the obvious advantage that Classifier-Free Guidance requires training only one model instead of two, the GLIDE authors additionally note that it empirically outperforms Classifier Guidance in photorealism and text-image alignment according to human evaluation.</p><h2 id=references>References<a class=anchor href=#references>#</a></h2><p>[1] Dhariwal, P., & Nichol, A. (2021). Diffusion models beat GANs on image synthesis. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/2105.05233 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2105.05233</a></p><p>[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/2006.11239 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2006.11239</a></p><p>[3] Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/2207.12598 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2207.12598</a></p><p>[4] Luo, C. (2022). Understanding diffusion models: A unified perspective. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/2208.11970 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2208.11970</a></p><p>[5] Nichol, A., & Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/2102.09672 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2102.09672</a></p><p>[6] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., & Chen, M. (2021). GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In arXiv [cs.CV]. <a href=http://arxiv.org/abs/2112.10741 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2112.10741</a></p><p>[7] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with CLIP latents. In arXiv [cs.CV]. <a href=http://arxiv.org/abs/2204.06125 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2204.06125</a></p><p>[8] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. In arXiv [cs.CV]. <a href=http://arxiv.org/abs/2205.11487 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/2205.11487</a></p><p>[9] Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., & Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In arXiv [cs.LG]. <a href=http://arxiv.org/abs/1503.03585 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/1503.03585</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script></article><div class=pagination><a class=pagination__item href=https://milesvant.github.io/posts/alphagozero/><span class=pagination__label>Next Post</span>
<span class=pagination__title>AlphaGo and AlphaGo Zero</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link title=GitHub href=https://github.com/milesvant target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/github.svg)></div></a><a class=social-icons__link title=Email href=mailto:milesvant@gmail.com target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/email.svg)></div></a><a class=social-icons__link title=LinkedIn href=https://www.linkedin.com/in/miles-van-tongeren-55270b119/ target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://milesvant.github.io/svg/linkedin.svg)></div></a></div><p></p></footer></div></div><div class=toc-container><div class=toc-post-title>An Introduction to Diffusion Models</div><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#data-generation-process>Data Generation Process</a></li><li><a href=#loss-function>Loss Function</a></li><li><a href=#improvements>Improvements</a><ul><li><a href=#learning-the-variance>Learning the Variance</a></li><li><a href=#noise-schedule>Noise Schedule</a></li><li><a href=#strided-ancestral-sampling>Strided Ancestral Sampling</a></li></ul></li><li><a href=#conditioned-diffusion-models>Conditioned Diffusion Models</a><ul><li><a href=#classifier-guidance>Classifier Guidance</a></li><li><a href=#classifier-free-guidance>Classifier-Free Guidance</a></li><li><a href=#comparing-classifier-and-classifier-free-guidance>Comparing Classifier and Classifier-Free Guidance</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div></main><script src=/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script>
<script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script>
<script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script>
<script src=/js/table-of-contents.js></script></body></html>